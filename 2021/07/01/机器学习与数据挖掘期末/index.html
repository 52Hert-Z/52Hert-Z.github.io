<!DOCTYPE html><html lang="en_US" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>🧠💫机器学习与数据挖掘期末复习 | Starry</title><meta name="keywords" content="机器学习,期末复习"><meta name="author" content="小澈"><meta name="copyright" content="小澈"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="什么是机器学习 机器学习包括几个关键词：计算机、经验&#x2F;数据、模型、性能、预测 机器学习根据以往的经验和数据得到数学模型，并可以对未知数据进行预测，最终达到计算机算法和程序进行优化的目的 基本术语  泛化能力：机器学习的目标是让我们的模型能很好地应用于“新样本”，我们称模型适用于未知新样本的能力为泛化能力 监督学习：学习由一个输入到输出的映射，成为模型。模型的集合就是假设空间 半监督学习：具有少量标">
<meta property="og:type" content="article">
<meta property="og:title" content="🧠💫机器学习与数据挖掘期末复习">
<meta property="og:url" content="https://52hert-z.github.io/2021/07/01/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E6%9C%9F%E6%9C%AB/index.html">
<meta property="og:site_name" content="Starry">
<meta property="og:description" content="什么是机器学习 机器学习包括几个关键词：计算机、经验&#x2F;数据、模型、性能、预测 机器学习根据以往的经验和数据得到数学模型，并可以对未知数据进行预测，最终达到计算机算法和程序进行优化的目的 基本术语  泛化能力：机器学习的目标是让我们的模型能很好地应用于“新样本”，我们称模型适用于未知新样本的能力为泛化能力 监督学习：学习由一个输入到输出的映射，成为模型。模型的集合就是假设空间 半监督学习：具有少量标">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://52hert-z.github.io/img/ml1.jpeg">
<meta property="article:published_time" content="2021-06-30T16:00:00.000Z">
<meta property="article:modified_time" content="2021-07-05T13:58:46.016Z">
<meta property="article:author" content="小澈">
<meta property="article:tag" content="机器学习">
<meta property="article:tag" content="期末复习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://52hert-z.github.io/img/ml1.jpeg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://52hert-z.github.io/2021/07/01/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E6%9C%9F%E6%9C%AB/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//fonts.googleapis.com" crossorigin=""/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web&amp;display=swap" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"We didn't find any results for the search: ${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: 'days',
  date_suffix: {
    just: 'Just',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: {"limitCount":50,"languages":{"author":"Author: 小澈","link":"Link: ","source":"Source: Starry","info":"Copyright is owned by the author. For commercial reprints, please contact the author for authorization. For non-commercial reprints, please indicate the source."}},
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    jQuery: 'https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js',
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    },
    fancybox: {
      js: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js',
      css: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '🧠💫机器学习与数据挖掘期末复习',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2021-07-05 21:58:46'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const fontSizeVal = saveToLocal.get('global-font-size')
    if (fontSizeVal !== undefined) {
      document.documentElement.style.setProperty('--global-font-size', fontSizeVal + 'px')
    }
    })(window)</script><meta name="generator" content="Hexo 5.4.0"><link rel="alternate" href="/atom.xml" title="Starry" type="application/atom+xml">
</head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="author-avatar"><img class="avatar-img" src="/img/avatar.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">10</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">Tags</div><div class="length-num">8</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">Categories</div><div class="length-num">4</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/img/ml1.jpeg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">Starry</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> Search</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">🧠💫机器学习与数据挖掘期末复习</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2021-06-30T16:00:00.000Z" title="Created 2021-07-01 00:00:00">2021-07-01</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2021-07-05T13:58:46.016Z" title="Updated 2021-07-05 21:58:46">2021-07-05</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E5%AD%A6%E6%A0%A1%E8%AF%BE%E7%A8%8B/">学校课程</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="🧠💫机器学习与数据挖掘期末复习"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post View:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1>什么是机器学习</h1>
<p>机器学习包括几个关键词：计算机、经验/数据、模型、性能、预测<br>
机器学习根据以往的经验和数据得到数学模型，并可以对未知数据进行预测，最终达到计算机算法和程序进行优化的目的</p>
<h2 id="基本术语">基本术语</h2>
<ul>
<li>泛化能力：机器学习的目标是让我们的模型能很好地应用于“新样本”，我们称模型适用于未知新样本的能力为泛化能力</li>
<li>监督学习：学习由一个输入到输出的映射，成为模型。模型的集合就是假设空间</li>
<li>半监督学习：具有少量标注数据，大量未标注数据，利用未标注数据的信息，辅助标注数据进行监督学习</li>
<li>主动学习：属于半监督学习的一种。先训练一个初始模型，给没有标签的数据打上伪标签，然后再用伪标签和其他有标签的数据重新训练模型。重复上述过程</li>
<li>模型评估和模型选择：
<ul>
<li>训练误差：在训练过程中产生的误差</li>
<li>验证误差：在验证集上进行交叉验证选择参数（调参），最终模型在验证集上的误差就是验证误差。</li>
<li>测试误差：训练完毕、调参完毕的模型，在新的测试集上的误差，就是测试误差。</li>
<li>泛化误差：假如所有的数据来自一个整体，模型在这个整体上的误差，就是泛化误差。通常说来，测试误差的平均值或者说期望就是泛化误差。</li>
</ul>
</li>
</ul>
<p>综合来说，它们的大小关系为<br>
训练误差 &lt; 验证误差 &lt; 测试误差 ～= 泛化误差</p>
<ul>
<li>损失函数：损失函数通常是针对单个训练样本而言，给定一个模型输出$\hat{y}$和一个真实输出$y$，损失函数衡量它们之间的差距，输出一个实值损失$L=f(y_{i},\hat{y}_{i})$</li>
<li>测试数据集准确度：</li>
</ul>
<h1>KNN（K近邻算法）</h1>
<ul>
<li>已有一批有标签的数据</li>
<li>输入没有标签的数据后，需要对他们进行分类。将新数据的每个特征和已有标签数据计算相似度</li>
<li>选取k个相似度最高的样本，分别统计他们的标签出现次数，出现最多的那个分类作为新数据的分类</li>
</ul>
<h2 id="如何定义相似度指标">如何定义相似度指标</h2>
<ol>
<li>直线距离（欧氏距离）</li>
<li>街区距离（曼哈顿距离）</li>
<li>闵可夫斯基距离：$\sum_{i=1}^{k}(\left | x_{i}-y_{i} \right |^{q})^{1-q}$</li>
<li>余弦距离</li>
</ol>
<h2 id="如何确定k值">如何确定k值</h2>
<ol>
<li>经验：对于n个训练样本，设置k为$\sqrt{n}$</li>
<li>实践中会通过交叉验证来确定k值<br>
k折交叉验证：将数据集划分为k等分，一份为测试集，剩余的$k-1$份作为训练集，比如说第一次选取第一个$\frac{1}{k}$作为测试集，一直循环到最后一份，计算k次测试集上的正确率的平均值。</li>
</ol>
<h2 id="引入特征的重要性">引入特征的重要性</h2>
<p>计算相似度的时候将不同特征加权求和</p>
<h2 id="归一化">归一化</h2>
<p>不同特征指标单位不同，取值范围也不同，对距离的计算带来比较大的影响。使用minmax进行归一化，也就是将数据变成在区间$(0,1)$之间的小数。<br>
$$<br>
\frac{x-min(x)}{max(x)-min(x)}<br>
$$</p>
<ul>
<li>主要是为了数据处理方便提出来的，把数据映射到0～1范围之内处理，更加便捷快速</li>
<li>把有量纲表达式变成无量纲表达式，便于不同单位或量级的指标能够进行比较和加权。<br>
归一化是一种简化计算的方式，即将有量纲的表达式，经过变换，化为无量纲的表达式，成为纯量</li>
</ul>
<h2 id="标准化">标准化</h2>
<p>在机器学习中，我们可能要处理不同种类的资料，例如，音讯和图片上的像素值，这些资料可能是高维度的，资料标准化后会使每个特征中的数值平均变为0(将每个特征的值都减掉原始资料中该特征的平均)、标准差变为1，这个方法被广泛的使用在许多机器学习算法中(例如：支持向量机、逻辑回归和类神经网络)。</p>
<h2 id="中心化">中心化</h2>
<p>平均值为0，对标准差无要求</p>
<h2 id="归一化和标准化的区别">归一化和标准化的区别</h2>
<p>归一化是将样本的特征值转换到同一量纲下把数据映射到[0,1]或者[-1, 1]区间内，仅由变量的极值决定，因区间放缩法是归一化的一种。标准化是依照特征矩阵的列处理数据，其通过求z-score的方法，转换为标准正态分布，和整体样本分布相关，每个样本点都能对标准化产生影响。它们的相同点在于都能取消由于量纲不同引起的误差；都是一种线性变换，都是对向量X按照比例压缩再进行平移。</p>
<h2 id="标准化和中心化的区别">标准化和中心化的区别</h2>
<p>标准化是原始分数减去平均数然后除以标准差，中心化是原始分数减去平均数。 所以一般流程为先中心化再标准化。</p>
<h2 id="无量纲">无量纲</h2>
<p>我的理解就是通过某种方法能去掉实际过程中的单位，从而简化计算。</p>
<h2 id="KNN优点">KNN优点</h2>
<ol>
<li>很简单，容易实现</li>
<li>可以应用于任何分布的情况</li>
<li>样本数量很大也能产生好的结果</li>
</ol>
<h2 id="KNN缺点">KNN缺点</h2>
<ol>
<li>参数的选择，k值</li>
<li>对分类新的样本时间比较长，因为要计算大量的距离</li>
</ol>
<h1>决策树</h1>
<h2 id="CLS算法">CLS算法</h2>
<p>早期的决策树学习算法，是许多决策树学习算法的基础，他没有规定策略去选择属性</p>
<h2 id="ID3算法">ID3算法</h2>
<p>使用信息增益选择测试属性<br>
$$<br>
H(x)=-\sum p_{i}log(p_{i})<br>
$$<br>
$$<br>
G(D,a)=H(D)-\sum_{v=1}^{V}\frac{|D^{v}|}{|D|}log_{2}\frac{|D^{v}|}{|D|}<br>
$$<br>
信息增益倾向于选择取值较多的特征，因此我们对信息增益归一化，引入增益率<br>
也即是C4.5算法<br>
$$<br>
Gain_ratio(D,a)=\frac{Gain(D,a)}{IV(D,a)}<br>
$$<br>
$$<br>
IV(a)=-\sum_{v=1}^{V}\frac{|D^{v}|}{|D|}log_{2}\frac{|D^{v}|}{|D|}<br>
$$<br>
不过还存在的问题变成了倾向于取值数目比较少的属性<br>
ID3算法的优点是：算法的理论清晰，方法简单，学习能力较强。其缺点是：只对比较小的数据集有效，且对噪声比较敏感，当训练数据集加大时，决策树可能会随之改变。</p>
<h2 id="启发式方法">启发式方法</h2>
<p>一种启发式方法是结合二者，先从候选划分属性中找出信息增益高于平均水平的属性，再从中选择增益率最高的</p>
<h2 id="CART算法">CART算法</h2>
<p>使用Gini指标进行属性选择<br>
Gini值：反映了从D中随机抽取两个样本，其类别标记不一致的概率，<strong>因此Gini指标越小，代表这样分类的效果越好</strong><br>
$$<br>
Gini(D)=\sum_{k=1}^{|\gamma|}\sum_{k’\neq k}p_{k}p_{k’}=1-\sum_{k=1}^{|\gamma|}p_{k}^{2}<br>
$$<br>
$$<br>
Gini_index(D,a)=\sum_{v=1}^{V}\frac{|D^{v}|}{|D|}Gini(D^{v})<br>
$$<br>
对于连续的问题，可以得到回归树：使用平方误差最小化作为衡量指标选择属性。对划分出来的样本集，计算他们的方差，目标是选取划分之后方差最小的划分方法。</p>
<h2 id="决策树过拟合问题">决策树过拟合问题</h2>
<p>先从数据集中划分出一部分验证集作为评估指标</p>
<ol>
<li>预剪枝：在模型构建过程中对模型进行剪枝<br>
在决策树生成过程中，对每个节点在划分前进行验证集的估计，如果划分节点对泛化能力不带来提升，则停止划分并将当前节点记为叶节点</li>
<li>后剪枝：构建完之后再进行剪枝，方法是去掉每个节点，看能否带来泛化能力的提升，如果可以就去掉</li>
</ol>
<h2 id="连续值的处理">连续值的处理</h2>
<p>连续属性a在样本集中出现n个不同的取值，我们从小到大排列，对他们进行二分划分为两部分，划分的时候取中间值</p>
<h1>集成学习</h1>
<h2 id="bagging">bagging</h2>
<p>bagging是投票式算法，使用bootstrap产生训练集，从D中有放回抽样，分别基于这些训练数据集得到多个基础分类器，最后通过这些分类器得出模型，这些模型同时进行预测，选择得票最多的那个分类</p>
<h3 id="bagging的特点">bagging的特点</h3>
<ul>
<li>个体学习期之间不存在强依赖关系，具有独立性</li>
<li>可以并行生成</li>
<li>算法的复杂度低，训练一个bagging集成与直接使用基学习器的复杂度同阶</li>
</ul>
<h3 id="使用场景">使用场景</h3>
<p>单个模型不稳定，用集成的方法选择最适合的</p>
<h3 id="训练算法">训练算法</h3>
<p>通过对D有放回抽样，得到t个训练集$D_{t}$，使用$D_{t}$进行训练得到模型$M_{t}$。预测的时候使用组合分类器对样本进行分类，T个模型同时预测，并返回多数的表决</p>
<h2 id="随机森林">随机森林</h2>
<h3 id="随机森林的随机性">随机森林的随机性</h3>
<ol>
<li>对样本进行有放回的抽样</li>
<li>在对每个点划分的时候，随机选择特征进行考虑</li>
<li>样本在某个属性有多个特征值的时候，随机划分进一个特征值的分类中</li>
</ol>
<h3 id="算法">算法</h3>
<ol>
<li>
<p>用n来表示训练样本的个数，d表示特征数目</p>
</li>
<li>
<p>输入特征数目m，用于确定决策树划分属性个数，其中m应远小于d</p>
</li>
<li>
<p>从n个训练样本中以有放回抽样的方式，取样n次，形成一个训练集</p>
</li>
<li>
<p>对于每一个节点，随机选择m个特征，决策树上每个节点的决定都是基于这些特征确定的，根据这m个特征，计算其最佳的分裂方式</p>
</li>
<li>
<p>从数据集（表）中随机选择k个特征（列），共m个特征（其中k小于等于m）。然后根据这k个特征建立决策树。2. 重复n次，这k个特性经过不同随机组合建立起来n棵决策树（或者是数据的不同随机样本，称为自助法样本）。3. 对每个决策树都传递随机变量来预测结果。存储所有预测的结果（目标），你就可以从n棵决策树中得到n种结果。4. 计算每个预测目标的得票数再选择模式（最常见的目标变量）。换句话说，将得到高票数的预测目标作为随机森林算法的最终预测。</p>
</li>
</ol>
<p>优点：</p>
<ol>
<li>可以用来解决分类和回归问题：随机森林可以同时处理分类和数值特征。</li>
<li>抗过拟合能力：通过平均决策树，降低过拟合的风险性。决策树使用贪婪的思想，容易陷入局部最优，随机森林的随机性可以跳出局部最优。</li>
<li>只有在半数以上的基分类器出现差错时才会做出错误的预测：随机森林非常稳定，即使数据集中出现了一个新的数据点，整个算法也不会受到过多影响，它只会影响到一颗决策树，很难对所有决策树产生影响。</li>
<li>处理高维数据，不用进行特征选择</li>
<li>容易做成并行化方法<br>
缺点：</li>
<li>据观测，如果一些分类/回归问题的训练数据中存在噪音，随机森林中的数据集会出现过拟合的现象。</li>
<li>比决策树算法更复杂，计算成本更高。</li>
<li>由于其本身的复杂性，它们比其他类似的算法需要更多的时间来训练。</li>
</ol>
<p>评价指标：分类间隔，对样本A有75%的分类正确，则分类间隔为75-25=50%<br>
袋外错误率：对每⼀棵树来说，都有样本没有被抽样进⼊训练样本中，这些就是袋外样本。对袋外样本<br>
预测的错误率就是袋外错误率。</p>
<h2 id="boosting">boosting</h2>
<p>Boosting是一种通过组合弱学习器来产生强学习器的通用且有效的方法。本文中我们将重点讲解三种Boosting算法:AdaBoost, RankBoost, Gradient Boosting。AdaBoost是第一个成功的Boosting算法,所以我们先介绍AdaBoost</p>
<h3 id="adaboost">adaboost</h3>
<p>AdaBoost解决了如下两个问题:首先,如何选择一组有不同优缺点的弱学习器,使得它们可以相互弥补不足。其次,如何组合弱学习器的输出以获得整体的更好的决策表现</p>
<p>我自己的理解：<br>
adaboost对每个样本都维护一个权重分布，每个样本都有一个权重值表示它的重要性，在衡量错误率的时候，权重大的样本分类错误贡献了比权重小的更大的错误率。这使得弱分类器要聚焦于这些高权重的样本，引导这些弱学习器学习训练样本的不同部分，我们进行多轮训练，在每一轮训练中都更新权重分布，对那些误分类的样本提高权重，那些上一轮分类准确的就降低权重，这样每一轮弱学习器都有进步。<br>
训练完之后，我们就得到了一组具有不同优缺点的弱学习器，给他们加权使它们的优势组合起来。</p>
<h3 id="bagging和boosting的区别">bagging和boosting的区别</h3>
<ol>
<li>样本选择：</li>
</ol>
<ul>
<li>bagging：训练集有放回抽样，从原始集中选出的各轮训练集是互相独立的</li>
<li>boosting：每一轮的训练集都一样只是样本在各个弱学习器中的权重发生变化</li>
</ul>
<ol start="2">
<li>样例权重：</li>
</ol>
<ul>
<li>bagging：每个样本的权重相等</li>
<li>boosting：根据错误率不断调整样本的权重，聚焦于分类错的样本上</li>
</ul>
<ol start="3">
<li>预测函数：</li>
</ol>
<ul>
<li>bagging：所有预测函数的权重相等</li>
<li>boosting：每个弱分类器都有相应的权重，对于分类误差小的分类器会有更大的权重</li>
</ul>
<ol start="4">
<li>并行计算</li>
</ol>
<ul>
<li>bagging：各个预测函数可以并行生成</li>
<li>boosting：各个预测函数只能顺序生成，因为后一个模型的权重需要根据上一轮的分类结果</li>
</ul>
<h1>线性模型</h1>
<h2 id="线性判别分析LDA">线性判别分析LDA</h2>
<p>LDA是一种监督学习的降维技术，也就是说它的数据集的每个样本是有类别输出的。这点和PCA不同。PCA是不考虑样本类别输出的无监督降维技术。LDA的思想可以用一句话概括，就是“投影后类内方差最小，类间方差最大”。什么意思呢？ 我们要将数据在低维度上进行投影，投影后希望每一种类别数据的投影点尽可能的接近，而不同类别的数据的类别中心之间的距离尽可能的大。</p>
<p>可能还是有点抽象，我们先看看最简单的情况。假设我们有两类数据 分别为红色和蓝色，如下图所示，这些数据特征是二维的，我们希望将这些数据投影到一维的一条直线，让每一种类别数据的投影点尽可能的接近，而红色和蓝色数据中心之间的距离尽可能的大。<br>
<img src="/img/fx1.png" alt=""></p>
<h3 id="类内散度矩阵">类内散度矩阵</h3>
<p>衡量同一类内样本间的距离<br>
$$<br>
S_{w}=\sum_{0}+\sum_{1}=\sum_{x\in X_{0}}(x-\mu_{0})(x-\mu_{0})^{T}+\sum_{x\in X_{1}}(x-\mu_{1})(x-\mu_{1})^{T}<br>
$$</p>
<h3 id="类间散度矩阵">类间散度矩阵</h3>
<p>衡量不同类之间的距离<br>
$$<br>
S_{b}=(\mu_{0}-\mu_{1})(\mu_{0}-\mu_{1})^{T}<br>
$$</p>
<h3 id="最大化目标">最大化目标</h3>
<p>不同类别间距离最大，也就是投影后的样本他们的均值的方差最大</p>
<h2 id="多分类问题">多分类问题</h2>
<h1>支持向量机SVM</h1>
<p>⽀持向量机的⽬标，是找到⼀个超平⾯，能最好的分隔不同类别的样本。它的优化⽬标是：最⼤化样本<br>
中距离超平⾯最近的距离，约束条件是样本都被正确分隔。也就是：<br>
<img src="/img/fx2.png" alt=""></p>
<h2 id="对偶问题">对偶问题</h2>
<p>通过拉格朗⽇乘⼦法将原始问题转化为对偶问题，通过解对偶问题得到原始问题的解。</p>
<h2 id="核函数">核函数</h2>
<hr>
<p>[随机森林][<a target="_blank" rel="noopener" href="https://www.zhihu.com/question/64043740/answer/672248748">https://www.zhihu.com/question/64043740/answer/672248748</a>]</p>
</article><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a><a class="post-meta__tags" href="/tags/%E6%9C%9F%E6%9C%AB%E5%A4%8D%E4%B9%A0/">期末复习</a></div><div class="post_share"><div class="social-share" data-image="/img/ml1.jpeg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2021/07/01/%E6%94%BF%E6%B2%BB%E9%9A%8F%E6%83%B3/"><img class="prev-cover" src="/img/poli1.jpeg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">Previous Post</div><div class="prev_info">🤔关于政治的一些思考（仅代表个人观点）</div></div></a></div><div class="next-post pull-right"><a href="/2021/07/01/%E8%8B%B1%E8%AF%AD/"><img class="next-cover" src="/img/en1.jpeg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">Next Post</div><div class="next_info">英语生词✏️</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span> Related Articles</span></div><div class="relatedPosts-list"><div><a href="/2021/07/05/机器/" title="🧠💫机器学习与数据挖掘期末复习（二）"><img class="cover" src="/img/ml1.jpeg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-07-05</div><div class="title">🧠💫机器学习与数据挖掘期末复习（二）</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Catalog</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">1.</span> <span class="toc-text">什么是机器学习</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9F%BA%E6%9C%AC%E6%9C%AF%E8%AF%AD"><span class="toc-number">1.1.</span> <span class="toc-text">基本术语</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">2.</span> <span class="toc-text">KNN（K近邻算法）</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A6%82%E4%BD%95%E5%AE%9A%E4%B9%89%E7%9B%B8%E4%BC%BC%E5%BA%A6%E6%8C%87%E6%A0%87"><span class="toc-number">2.1.</span> <span class="toc-text">如何定义相似度指标</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A6%82%E4%BD%95%E7%A1%AE%E5%AE%9Ak%E5%80%BC"><span class="toc-number">2.2.</span> <span class="toc-text">如何确定k值</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BC%95%E5%85%A5%E7%89%B9%E5%BE%81%E7%9A%84%E9%87%8D%E8%A6%81%E6%80%A7"><span class="toc-number">2.3.</span> <span class="toc-text">引入特征的重要性</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BD%92%E4%B8%80%E5%8C%96"><span class="toc-number">2.4.</span> <span class="toc-text">归一化</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A0%87%E5%87%86%E5%8C%96"><span class="toc-number">2.5.</span> <span class="toc-text">标准化</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%AD%E5%BF%83%E5%8C%96"><span class="toc-number">2.6.</span> <span class="toc-text">中心化</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BD%92%E4%B8%80%E5%8C%96%E5%92%8C%E6%A0%87%E5%87%86%E5%8C%96%E7%9A%84%E5%8C%BA%E5%88%AB"><span class="toc-number">2.7.</span> <span class="toc-text">归一化和标准化的区别</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A0%87%E5%87%86%E5%8C%96%E5%92%8C%E4%B8%AD%E5%BF%83%E5%8C%96%E7%9A%84%E5%8C%BA%E5%88%AB"><span class="toc-number">2.8.</span> <span class="toc-text">标准化和中心化的区别</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%97%A0%E9%87%8F%E7%BA%B2"><span class="toc-number">2.9.</span> <span class="toc-text">无量纲</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#KNN%E4%BC%98%E7%82%B9"><span class="toc-number">2.10.</span> <span class="toc-text">KNN优点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#KNN%E7%BC%BA%E7%82%B9"><span class="toc-number">2.11.</span> <span class="toc-text">KNN缺点</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">3.</span> <span class="toc-text">决策树</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#CLS%E7%AE%97%E6%B3%95"><span class="toc-number">3.1.</span> <span class="toc-text">CLS算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#ID3%E7%AE%97%E6%B3%95"><span class="toc-number">3.2.</span> <span class="toc-text">ID3算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%90%AF%E5%8F%91%E5%BC%8F%E6%96%B9%E6%B3%95"><span class="toc-number">3.3.</span> <span class="toc-text">启发式方法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#CART%E7%AE%97%E6%B3%95"><span class="toc-number">3.4.</span> <span class="toc-text">CART算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%86%B3%E7%AD%96%E6%A0%91%E8%BF%87%E6%8B%9F%E5%90%88%E9%97%AE%E9%A2%98"><span class="toc-number">3.5.</span> <span class="toc-text">决策树过拟合问题</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%BF%9E%E7%BB%AD%E5%80%BC%E7%9A%84%E5%A4%84%E7%90%86"><span class="toc-number">3.6.</span> <span class="toc-text">连续值的处理</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">4.</span> <span class="toc-text">集成学习</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#bagging"><span class="toc-number">4.1.</span> <span class="toc-text">bagging</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#bagging%E7%9A%84%E7%89%B9%E7%82%B9"><span class="toc-number">4.1.1.</span> <span class="toc-text">bagging的特点</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8%E5%9C%BA%E6%99%AF"><span class="toc-number">4.1.2.</span> <span class="toc-text">使用场景</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E7%AE%97%E6%B3%95"><span class="toc-number">4.1.3.</span> <span class="toc-text">训练算法</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97"><span class="toc-number">4.2.</span> <span class="toc-text">随机森林</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%E7%9A%84%E9%9A%8F%E6%9C%BA%E6%80%A7"><span class="toc-number">4.2.1.</span> <span class="toc-text">随机森林的随机性</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AE%97%E6%B3%95"><span class="toc-number">4.2.2.</span> <span class="toc-text">算法</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#boosting"><span class="toc-number">4.3.</span> <span class="toc-text">boosting</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#adaboost"><span class="toc-number">4.3.1.</span> <span class="toc-text">adaboost</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#bagging%E5%92%8Cboosting%E7%9A%84%E5%8C%BA%E5%88%AB"><span class="toc-number">4.3.2.</span> <span class="toc-text">bagging和boosting的区别</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">5.</span> <span class="toc-text">线性模型</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E5%88%A4%E5%88%AB%E5%88%86%E6%9E%90LDA"><span class="toc-number">5.1.</span> <span class="toc-text">线性判别分析LDA</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%B1%BB%E5%86%85%E6%95%A3%E5%BA%A6%E7%9F%A9%E9%98%B5"><span class="toc-number">5.1.1.</span> <span class="toc-text">类内散度矩阵</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%B1%BB%E9%97%B4%E6%95%A3%E5%BA%A6%E7%9F%A9%E9%98%B5"><span class="toc-number">5.1.2.</span> <span class="toc-text">类间散度矩阵</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9C%80%E5%A4%A7%E5%8C%96%E7%9B%AE%E6%A0%87"><span class="toc-number">5.1.3.</span> <span class="toc-text">最大化目标</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%9A%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98"><span class="toc-number">5.2.</span> <span class="toc-text">多分类问题</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">6.</span> <span class="toc-text">支持向量机SVM</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AF%B9%E5%81%B6%E9%97%AE%E9%A2%98"><span class="toc-number">6.1.</span> <span class="toc-text">对偶问题</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A0%B8%E5%87%BD%E6%95%B0"><span class="toc-number">6.2.</span> <span class="toc-text">核函数</span></a></li></ol></li></ol></div></div></div></div></main><footer id="footer" style="background-image: url('/img/ml1.jpeg')"><div id="footer-wrap"><div class="copyright">&copy;2021 By 小澈</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="font-plus" type="button" title="Increase font size"><i class="fas fa-plus"></i></button><button id="font-minus" type="button" title="Decrease font size"><i class="fas fa-minus"></i></button><button id="darkmode" type="button" title="Toggle Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between single-column and double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back To Top"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><div class="search-dialog__title" id="local-search-title">Local search</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts" type="text"/></div></div></div><hr/><div id="local-search-results"></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script>function panguFn () {
  if (typeof pangu === 'object') pangu.autoSpacingPage()
  else {
    getScript('https://cdn.jsdelivr.net/npm/pangu/dist/browser/pangu.min.js')
      .then(() => {
        pangu.autoSpacingPage()
      })
  }
}

function panguInit () {
  if (false){
    GLOBAL_CONFIG_SITE.isPost && panguFn()
  } else {
    panguFn()
  }
}

document.addEventListener('DOMContentLoaded', panguInit)</script><script src="/js/search/local-search.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    loader: {
      source: {
        '[tex]/amsCd': '[tex]/amscd'
      }
    },
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        addClass: [200,() => {
          document.querySelectorAll('mjx-container:not([display=\'true\']').forEach( node => {
            const target = node.parentNode
            if (!target.classList.contains('has-jax')) {
              target.classList.add('mathjax-overflow')
            }
          })
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script><script>if (document.getElementsByClassName('mermaid').length) {
  if (window.mermaidJsLoad) mermaid.init()
  else {
    getScript('https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js').then(() => {
      window.mermaidJsLoad = true
      mermaid.initialize({
        theme: 'default',
      })
      false && mermaid.init()
    })
  }
}</script></div><script id="click-heart" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/click-heart.min.js" async="async" mobile="false"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>